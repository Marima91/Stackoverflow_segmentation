{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from nlp_module import normalize_corpus, remove_stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import gc\n",
    "import time\n",
    "from langdetect import detect\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import re\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install langdetect\n",
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation et description des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai téléchargé les données à travers la requete SQL suivante sur le site: https://data.stackexchange.com/stackoverflow/query/\n",
    "\n",
    "SELECT Title, Body, Tags, Id, Score, ViewCount, FavoriteCount, AnswerCount\n",
    "\n",
    "FROM Posts \n",
    "\n",
    "WHERE PostTypeId = 1 AND ViewCount > 0 AND FavoriteCount > 0\n",
    "\n",
    "AND Score > 0 AND AnswerCount > 0 AND LEN(Tags) - LEN(REPLACE(Tags, '<','')) >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"QueryResults .csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((data.isnull().mean()\\\n",
    "       *100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le jeu de données ne compte pas de valeurs nulles. La variable Id ne compte que des valeurs uniques, nous pouvons donc l'utiliser en index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('Id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons vérifier la longeur des différents titres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 12))\n",
    "ax = sns.countplot(x=data.Title.str.len())\n",
    "start, end = data.Title.str.len().min(),data.Title.str.len().max()\n",
    "ax.xaxis.set_ticks([])#np.arange(0, end, 5))\n",
    "plt.axvline(data.Title.str.len().median() - start,\n",
    "            color=\"r\", linestyle='--',\n",
    "            label=\"Title Lenght median : \"+str(data.Title.str.len().median()))\n",
    "ax.set_xlabel(\"Lenght of title\")\n",
    "plt.title(\"Title lenght of Stackoverflow questions\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons faire une rapide analyse exploratoire sur les tags de notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tags'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons modifier les séparateurs de Tags pour favoriser les extractions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace open and close balise between tags\n",
    "data['Tags'] = data['Tags'].str.translate(str.maketrans({'<': '', '>': ','}))\n",
    "\n",
    "# Delete last \",\" for each row\n",
    "data['Tags'] = data['Tags'].str[:-1]\n",
    "data['Tags'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les tags contenus dans la variable Tags sont ensuite splités et ajoutés dans une liste pour ensuite les classer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_split_tags(df, column, separator):\n",
    "    \"\"\"This function allows you to split the different words contained\n",
    "    in a Pandas Series cell and to inject them separately into a list.\n",
    "    This makes it possible, for example, to count the occurrences of words.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    df : Pandas Dataframe\n",
    "        Dataframe to use.\n",
    "    column : string\n",
    "        Column of the dataframe to use\n",
    "    separator : string\n",
    "        Separator character for str.split.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    list_words = []\n",
    "    for word in df[column].str.split(separator):\n",
    "        list_words.extend(word)\n",
    "    df_list_words = pd.DataFrame(list_words, columns=[\"Tag\"])\n",
    "    df_list_words = df_list_words.groupby(\"Tag\")\\\n",
    "        .agg(tag_count=pd.NamedAgg(column=\"Tag\", aggfunc=\"count\"))\n",
    "    df_list_words.sort_values(\"tag_count\", ascending=False, inplace=True)\n",
    "    return df_list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = count_split_tags(df=data, column='Tags', separator=',')\n",
    "print(\"Le jeu de données compte {} tags.\".format(tags_list.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of splits\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "sns.barplot(data=tags_list.iloc[0:40, :],\n",
    "            x=tags_list.iloc[0:40, :].index,\n",
    "            y=\"tag_count\", color=\"#f48023\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"40 most popular tags in Stackoverflow\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les 40 tags les plus populaires sur StackOverflow, les tags  C#, java et javascript sont sans surprise dans le top 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également ***visualiser les 500 premières catégories dans un nuage de mots*** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word cloud with tags_list (frequencies)\n",
    "fig = plt.figure(1, figsize=(17, 12))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "wordcloud = WordCloud(width=900, height=500,\n",
    "                      background_color=\"black\",\n",
    "                      max_words=500, relative_scaling=1,\n",
    "                      normalize_plurals=False)\\\n",
    "    .generate_from_frequencies(tags_list.to_dict()['tag_count'])\n",
    "\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\")\n",
    "plt.title(\"Word Cloud of 500 best Tags on StackOverflow\\n\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrage du jeu de données avec les meilleurs Tags :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les process de NLP sont des algorithmes assez lents compte tenu de la quantité de données à traiter. Pour filtrer notre jeu de données, nous allons sélectionner toutes les questions qui comportent au minimum un des 50 meilleurs tags et supprimer les autres tags :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tag(x, top_list):\n",
    "    \"\"\"Comparison of the elements of 2 lists to \n",
    "    check if all the tags are found in a list of top tags.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    x : list\n",
    "        List of tags to test.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    temp_list = []\n",
    "    for item in x:\n",
    "        if (item in top_list):\n",
    "            #x.remove(item)\n",
    "            temp_list.append(item)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Tags_list'] = data['Tags'].str.split(',')\n",
    "top_tags = list(tags_list.iloc[0:50].index)\n",
    "data['Tags_list'] = data['Tags_list'].apply(lambda x: filter_tag(x, top_tags))\n",
    "data['number_of_tags'] = data['Tags_list'].apply(lambda x : len(x))\n",
    "data = data[data.number_of_tags > 0]\n",
    "print(\"New size of dataset : {} questions.\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_emb.csv\", index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage des questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de traiter au mieux les données textuelles du Body, il est nécessaire de réaliser plusieurs tâches de data cleaning. Par exemple, le texte stocké dans cette variable est au format HTML. Ces balises vont polluer notre analyse. Nous allons donc supprimer toutes les balises HTML avec la librairie BeautifulSoup pour ne conserver que le texte brut.\n",
    "\n",
    "Mais avant cette opération,nous allons supprimer tout le contenu placé entre 2 balises html <code></code>, cela nous permettra de supprimer tout le code brut souvent copié dans les questions Stackoverflow et qui pourrait avoir un fort impact pour la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup  \n",
    "def remove_code(x):\n",
    "    \"\"\"Function based on the Beautifulsoup library intended to replace \n",
    "    the content of all the <code> </code> tags of a text specified as a parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    x : string\n",
    "        Sequence of characters to modify.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(x,\"lxml\")\n",
    "    code_to_remove = soup.findAll(\"code\")\n",
    "    for code in code_to_remove:\n",
    "        code.replace_with(\" \")\n",
    "    return str(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Delete <code> in Body text\n",
    "data['Body'] = data['Body'].apply(remove_code)\n",
    "# Delete all html tags\n",
    "data['Body'] = [BeautifulSoup(text,\"lxml\").get_text() for text in data['Body']]\n",
    "exec_time = time.time() - start_time\n",
    "print('-' * 50)\n",
    "print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print('-' * 50)\n",
    "print(data['Body'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, nous devons ***vérifier si les textes des questions sont rédigés en diverses langues***. Cela nous permettra de définir la liste des stop words à éliminer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature \"lang\" with langdetect library\n",
    "def detect_lang(x):\n",
    "    try:\n",
    "        return detect(x)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "start_time = time.time()\n",
    "data['short_body'] = data['Body'].apply(lambda x: x[0:100])\n",
    "data['lang'] = data.short_body.apply(detect_lang)\n",
    "exec_time = time.time() - start_time\n",
    "print('-' * 50)\n",
    "print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count titles for each language\n",
    "df_lang=pd.DataFrame(data.lang.value_counts())\n",
    "df_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results of splits\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "#x= df_lang.index\n",
    "#y=df_lang[\"lang\"]\n",
    "sns.barplot(data=df_lang,\n",
    "           y=\"lang\",\n",
    "            x=df_lang.index,\n",
    "            color=\"#f48023\")\n",
    "#sns.barplot(x, y)\n",
    "plt.xticks()\n",
    "plt.title(\"Language of questions\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La langue Anglaise est très majoritairement représentée dans notre dataset. Nous allons donc ***supprimer de notre jeu de données tous les post dans une autre langue que l'anglais***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletion of data that is not in the English language\n",
    "data = data[data['lang']=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons un texte brut débarassé de ses balises HTML et du code, nous allons utiliser nltk.pos_tag pour identifier la nature de chaque mot du corpus afin de pouvoir ensuite conserver uniquement les noms. Nous allons ici créer une function qui sera appliquée ensuite dans un cleaner plus complet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pos(nlp, x, pos_list):\n",
    "    doc = nlp(x)\n",
    "    list_text_row = []\n",
    "    for token in doc:\n",
    "        if(token.pos_ in pos_list):\n",
    "            list_text_row.append(token.text)\n",
    "    join_text_row = \" \".join(list_text_row)\n",
    "    join_text_row = join_text_row.lower().replace(\"c #\", \"c#\")\n",
    "    return join_text_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons à présent réaliser plusieurs opérations de Text cleaning pour que nos données soient exploitables par les algorithmes de NLP :\n",
    "\n",
    "Suppression de tous les mots autres que ***les noms***\n",
    "\n",
    "Mettre tout le texte en ***minuscules***\n",
    "\n",
    "Supprimer les ***caractères Unicode*** (comme les Emojis par exemple)\n",
    "\n",
    "Suppression des ***espaces supplémentaires***\n",
    "\n",
    "Suppression de la ***ponctuation***\n",
    "\n",
    "Suppression des ***liens***\n",
    "\n",
    "Supprimer les ***nombres***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(x, nlp, pos_list):\n",
    "    \"\"\"Function allowing to carry out the preprossessing on the textual data. \n",
    "        It allows you to remove extra spaces, unicode characters, \n",
    "        English contractions, links, punctuation and numbers.\n",
    "        \n",
    "        The re library for using regular expressions must be loaded beforehand.\n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    x : string\n",
    "        Sequence of characters to modify.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    # Remove POS not in \"NOUN\", \"PROPN\"\n",
    "    x = remove_pos(nlp, x, pos_list)\n",
    "    # Case normalization\n",
    "    x = x.lower()\n",
    "    # Remove unicode characters\n",
    "    x = x.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Remove English contractions\n",
    "    x = re.sub(\"\\'\\w+\", '', x)\n",
    "    # Remove ponctuation but not # (for C# for example)\n",
    "    x = re.sub('[^\\\\w\\\\s#]', '', x)\n",
    "    # Remove links\n",
    "    x = re.sub(r'http*\\S+', '', x)\n",
    "    # Remove numbers\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    # Remove extra spaces\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    \n",
    "    # Return cleaned text\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy \n",
    "#import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "\n",
    "# Download spaCy's  'en' Model\n",
    "#!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply cleaner on Body\n",
    "# Spacy features\n",
    "nlp = spacy.load('en_core_web_sm', exclude=['ner', 'parser'])\n",
    "pos_list = [\"NOUN\",\"PROPN\"]\n",
    "\n",
    "start_time = time.time()\n",
    "print('-' * 50)\n",
    "print(\"Start Body cleaning ...\")\n",
    "print('-' * 50)\n",
    "\n",
    "tqdm.pandas()\n",
    "data['Body_cleaned'] = data.Body.progress_apply(lambda x : text_cleaner(x, nlp, pos_list))\n",
    "\n",
    "exec_time = time.time() - start_time\n",
    "print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print('-' * 50)\n",
    "print(data['Body_cleaned'].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Body_cleaned'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent ***supprimer tous les stop words en langue Anglaise*** grâce à la librairie NLTK. \n",
    "\n",
    "Avant cette étape, nous allons réaliser une ***tockenisation*** c'est à dire découper les phrase en mots et création d'une liste (chaque phrase est une liste de mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Tockenization\n",
    "data['Body_cleaned'] = data.Body_cleaned.apply(nltk.tokenize.word_tokenize)\n",
    "\n",
    "# List of stop words in \"EN\" from NLTK\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# Remove stop words\n",
    "data['Body_cleaned'] = data.Body_cleaned\\\n",
    "    .apply(lambda x : [word for word in x\n",
    "                       if word not in stop_words\n",
    "                       and len(word)>2])\n",
    "exec_time = time.time() - start_time\n",
    "print('-' * 50)\n",
    "print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print('-' * 50)\n",
    "print(data['Body_cleaned'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, nous avons des listes de mots débarrassées des mots courants (stop words), de la ponctuation, des liens et des nombres. Une dernière étape que nous pouvons effectuer est la Lemmatisation. Ce procédé consiste à prend le mot à sa forme racine appelée Lemme. Cela nous permet d'amener les mots à leur forme \"dictionnaire\". Nous allons pour cela utiliser à nouveau la librairie NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# Apply lemmatizer on Body\n",
    "start_time = time.time()\n",
    "wn = WordNetLemmatizer()\n",
    "data['Body_cleaned'] = data.Body_cleaned\\\n",
    "    .apply(lambda x : [wn.lemmatize(word) for word in x])\n",
    "exec_time = time.time() - start_time\n",
    "print('-' * 50)\n",
    "print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print('-' * 50)\n",
    "print(data['Body_cleaned'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des titres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons préalablement défini une fonction pour notre cleaning des Body. Nous allons la modifier pour y intégrer la tokenisation, les stop words et la lemmanisation afin d'obtenir un processus complet à appliquer aux titres des posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(x, nlp, pos_list, lang=\"english\"):\n",
    "    \"\"\"Function allowing to carry out the preprossessing on the textual data. \n",
    "        It allows you to remove extra spaces, unicode characters, \n",
    "        English contractions, links, punctuation and numbers.\n",
    "        \n",
    "        The re library for using regular expressions must be loaded beforehand.\n",
    "        The SpaCy and NLTK librairies must be loaded too. \n",
    "\n",
    "    Parameters\n",
    "    ----------------------------------------\n",
    "    x : string\n",
    "        Sequence of characters to modify.\n",
    "    ----------------------------------------\n",
    "    \"\"\"\n",
    "    # Remove POS not in \"NOUN\", \"PROPN\"\n",
    "    x = remove_pos(nlp, x, pos_list)\n",
    "    # Case normalization\n",
    "    x = x.lower()\n",
    "    # Remove unicode characters\n",
    "    x = x.encode(\"ascii\", \"ignore\").decode()\n",
    "    # Remove English contractions\n",
    "    x = re.sub(\"\\'\\w+\", '', x)\n",
    "    # Remove ponctuation but not # (for C# for example)\n",
    "    x = re.sub('[^\\\\w\\\\s#]', '', x)\n",
    "    # Remove links\n",
    "    x = re.sub(r'http*\\S+', '', x)\n",
    "    # Remove numbers\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    # Remove extra spaces\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "        \n",
    "    # Tokenization\n",
    "    x = nltk.tokenize.word_tokenize(x)\n",
    "    # List of stop words in select language from NLTK\n",
    "    stop_words = stopwords.words(lang)\n",
    "    # Remove stop words\n",
    "    x = [word for word in x if word not in stop_words \n",
    "         and len(word)>2]\n",
    "    # Lemmatizer\n",
    "    wn = nltk.WordNetLemmatizer()\n",
    "    x = [wn.lemmatize(word) for word in x]\n",
    "    \n",
    "    # Return cleaned text\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy features\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=['ner', 'parser'])\n",
    "pos_list = [\"NOUN\",\"PROPN\"]\n",
    "# Apply full cleaner on Title\n",
    "print('-' * 50)\n",
    "print(\"Start Title cleaning ...\")\n",
    "print('-' * 50)\n",
    "start_time = time.time()\n",
    "data['Title_cleaned'] = data.Title\\\n",
    "                            .progress_apply(lambda x: \n",
    "                                            text_cleaner(x, nlp,\n",
    "                                                         pos_list,\n",
    "                                                         \"english\"\n",
    "                                                         ))\n",
    "exec_time = time.time() - start_time\n",
    "print(\"Execution time : {:.2f}s\".format(exec_time))\n",
    "print('-' * 50)\n",
    "print(data['Title_cleaned'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent projeter la distribution de la taille des tokens Title et le nuage de mots correspondant aux 500 meilleurs apparitions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate lenght of each list in Title\n",
    "data['Title_tokens_count'] = [len(_) for _ in data.Title_cleaned]\n",
    "\n",
    "# Countplot of Title lenght\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "ax = sns.countplot(x=data.Title_tokens_count)\n",
    "median_plot = data.Title_tokens_count.median()\n",
    "plt.axvline(median_plot - data.Title_tokens_count.min(),\n",
    "            color=\"r\", linestyle='--',\n",
    "            label=\"Title tokens Lenght median : \"+str(median_plot))\n",
    "ax.set_xlabel(\"Lenght of title tokens\")\n",
    "plt.title(\"Title tokens lenght of Stackoverflow questions after cleaning\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all tokens for Title\n",
    "full_corpus_t = []\n",
    "for i in data['Title_cleaned']:\n",
    "    full_corpus_t.extend(i)\n",
    "\n",
    "# Calculate distribition of words in Title token list\n",
    "title_dist = nltk.FreqDist(full_corpus_t)\n",
    "title_dist = pd.DataFrame(title_dist.most_common(500),\n",
    "                          columns=['Word', 'Frequency'])\n",
    "\n",
    "# Plot word cloud with tags_list (frequencies)\n",
    "fig = plt.figure(1, figsize=(17, 12))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "wordcloud = WordCloud(width=900, height=500,\n",
    "                      background_color=\"black\",\n",
    "                      max_words=500, relative_scaling=1,\n",
    "                      normalize_plurals=False)\\\n",
    "    .generate_from_frequencies(title_dist.set_index('Word').to_dict()['Frequency'])\n",
    "\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.axis(\"off\")\n",
    "plt.title(\"Word Cloud of 500 most popular words on Title feature\\n\",\n",
    "          fontsize=18, color=\"#641E16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export du dataset nettoyé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove calculated features\n",
    "data = data[['Title_cleaned',\n",
    "             'Body_cleaned',\n",
    "             'Score',\n",
    "             'Tags_list']]\n",
    "# Rename columns\n",
    "data = data.rename(columns={'Title_cleaned': 'Title',\n",
    "                            'Body_cleaned': 'Body',\n",
    "                            'Tags_list': 'Tags'})\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_cleaned.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
